# -*- coding: utf-8 -*-
"""PrepareData.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jEIYt7CwktQz3Lz-4CQQfga-Zw86k7CJ
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.manifold import TSNE
from matplotlib import pyplot
from mpl_toolkits.mplot3d import Axes3D
from matplotlib.ticker import NullFormatter
import numpy as np
from sklearn.decomposition import PCA
from numpy.random import rand
import sklearn
import scipy
from sklearn.metrics import classification_report,accuracy_score
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor
from sklearn.svm import OneClassSVM
# %matplotlib inline
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix
from sklearn.svm import SVC
from sklearn.svm import SVR
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
import math
from pylab import rcParams
from sklearn.model_selection import train_test_split

dataset = pd.read_csv('/content/CreditCard/creditcard.csv', sep=",")

dataset.shape

dataset.head()

dataset.info()

x = dataset.iloc[: , 1:30].values
y = dataset.iloc[:, 30].values

print("Input Range : ", x.shape)
print("Output Range : ", y.shape)

print ("Class Labels : \n", y)

dataset.isnull().values.any()

print ("Not-Fraud")
print (dataset.Amount.describe())

d1 = dataset.drop(['Class'], axis=1)

d1.describe()

print ("Not-Fraud")
print (dataset.Time[dataset.Class == 0].describe())

print ("Fraud")
print (dataset.Time[dataset.Class == 1].describe())
print ()

label = ['NotFraud', 'Fraud']

#count_class = pd.value_counts(dataset['Class'], sort = True)
#count_class.plot(kind = 'pie', figsize=(5,5))
#plt.xticks(range(2), label)
#plt.tick_params(axis='x')
#plt.title("Frequency of Fraud and Not-Fraud Class")
#plt.xlabel("Class")
#plt.ylabel("Frequency");

set_class = pd.value_counts(dataset['Class'], sort = True)

set_class.plot(kind = 'bar', rot=0)

plt.title("Class Distribution of Transaction")

plt.xticks(range(2), label)

plt.xlabel("Classes")

plt.ylabel("No of occurences")

fraud_i=dataset[dataset.Class==1]
notfraud_i=dataset[dataset.Class==0]

print(fraud_i.shape,notfraud_i.shape)

fraud_i.Amount.describe()

notfraud_i.Amount.describe()

fraud_num=len(fraud_i)
notfraud_num=len(notfraud_i)

fraud_perc=round(fraud_num/(fraud_num+notfraud_num),5)*100
print("The percentage of fraud of all transactions is ", fraud_perc, "%")

notFraud = dataset[dataset.Class == 0] 
fraud = dataset[dataset.Class == 1] 

second_to_hour=3600

plt.figure(figsize=(17,12))
plt.scatter((notFraud.Time/(second_to_hour)), notFraud.Amount, alpha=0.6, label='Not-Fraud')
plt.scatter((fraud.Time/(second_to_hour)), fraud.Amount, alpha=0.9, label='Fraud')
plt.title("Amount of transaction by hour")
plt.xlabel("Transaction time ")
plt.ylabel('Amount (USD)')
plt.legend(loc='upper right')
plt.show()

bins = np.linspace(0, 48, 48) #48 hours
plt.figure(figsize=(15,12))

plt.hist((notFraud.Time/(second_to_hour)), bins, alpha=1, label='Not-Fraud')
plt.hist((fraud.Time/(second_to_hour)), bins, alpha=1, label='Fraud')

plt.legend(loc='upper right')
plt.title("Percentage of transactions by hour")
plt.xlabel("Transaction time as measured from first transaction in the dataset (hours)")
plt.ylabel("Percentage of transactions (%)");

plt.show()

f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(12,4))

bins = 30

ax1.hist(dataset.Amount[dataset.Class == 1], bins = bins, color='blue')
ax1.set_title('Fraud (1)')

ax2.hist(dataset.Amount[dataset.Class == 0], bins = bins, color='orange')
ax2.set_title('Not-Fraud (0)')

plt.xlabel('Amount ($)')
plt.ylabel('Number of Transactions')
plt.yscale('log')
plt.show()

print('Max Amount to Not-Fraud Detection is: $',max(dataset.Amount[dataset.Class==0]))        #max Amount($) Not-Fraud Transaction
print('Max Amount to Fraud Detection is: $',max(dataset.Amount[dataset.Class==1]))        #max Amount($) Fraud Transaction

dataset = pd.read_csv('../content/CreditCard/creditcard.csv', sep=",")

datasetForCorrelation = dataset.drop(['Class'], axis=1)

# Correlation matrix

corrmat = datasetForCorrelation.corr()


fig = plt.figure(figsize = (20, 25))

sns.heatmap(corrmat, vmax=1.0, center=0, fmt='.2f', cmap='PuRd_r',
            square=True, linewidths=.5, annot=True, cbar_kws={"shrink": .70})
plt.show()

#correlation matrix 
f, (ax1, ax2) = plt.subplots(1,2,figsize =( 15, 7 ))

sns.heatmap(dataset.query('Class==1').drop(['Class','Time'],1).corr(), vmax = .8, square=True, ax = ax1, cmap = 'YlGnBu')
ax1.set_title('Fraud')

sns.heatmap(dataset.query('Class==0').drop(['Class','Time'],1).corr(), vmax = .8, square=True, ax = ax2, cmap = 'YlGnBu');
ax2.set_title('Not Fraud')

plt.show()

xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.25, random_state = 0)

print("xtrain.shape : ", xtrain.shape)
print("xtest.shape  : ", xtest.shape)
print("ytrain.shape : ", ytrain.shape)
print("ytest.shape  : ", ytest.shape)

stdsc = StandardScaler()
xtrain = stdsc.fit_transform(xtrain)
xtest = stdsc.transform(xtest)

print("Training Set after Standardised : \n", xtrain[0])

dt_classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)
dt_classifier.fit(xtrain, ytrain)

y_pred_decision_tree = dt_classifier.predict(xtest)


print("y_pred_decision_tree : \n", y_pred_decision_tree)

import itertools

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=0)
    plt.yticks(tick_marks, classes)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        #print("Normalized confusion matrix")
    else:
        1#print('Confusion matrix, without normalization')

    #print(cm)

    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j],
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

com_decision = confusion_matrix(ytest, y_pred_decision_tree)
class_names = [0,1]
plt.figure()
plot_confusion_matrix(com_decision, classes=class_names, title='Confusion matrix')
plt.show()
print("confusion Matrix : \n", com_decision)

Accuracy_Model = ((com_decision[0][0] + com_decision[1][1]) / com_decision.sum()) *100
print("Accuracy_Decison    : ", Accuracy_Model)

Error_rate_Model= ((com_decision[0][1] + com_decision[1][0]) / com_decision.sum()) *100
print("Error_rate_Decison  : ", Error_rate_Model)

# True Fake Rate
Specificity_Model= (com_decision[1][1] / (com_decision[1][1] + com_decision[0][1])) *100
print("Specificity_Decison : ", Specificity_Model)

# True Genuine Rate
Sensitivity_Model = (com_decision[0][0] / (com_decision[0][0] + com_decision[1][0])) *100
print("Sensitivity_Decison : ", Sensitivity_Model)